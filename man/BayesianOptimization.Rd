% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BayesianOptimization.R
\name{BayesianOptimization}
\alias{BayesianOptimization}
\title{Bayesian Optimization}
\usage{
BayesianOptimization(FUN, bounds, saveIntermediate = NULL,
  leftOff = NULL, parallel = FALSE, packages = NULL, export = NULL,
  initialize = TRUE, initGrid = NULL, initPoints = 0, bulkNew = 1,
  nIters = 0, kern = "Matern52", beta = 0, acq = "ucb",
  stopImpatient = list(newAcq = "ucb", rounds = Inf), kappa = 2.576,
  eps = 0, gsPoints = 100, convThresh = 1e+07, noiseAdd = 0.25,
  verbose = 1)
}
\arguments{
\item{FUN}{the function to be maximized. This function should return a named list with at least 1 component.
The first component must be named \code{Score} and should contain the metric to be maximized.
The returned list may contain other named scalar elements that you wish to include in the final summary table.}

\item{bounds}{named list of lower and upper bounds for each hyperparameter.
The names of the list should be arguments passed to \code{FUN}.
Use "L" suffix to indicate integer hyperparameters.}

\item{saveIntermediate}{character filepath (including file name) that specifies the location to save intermediary results. This will
save a data.table as an RDS that can be specified as the \code{leftOff} parameter.}

\item{leftOff}{data.table containing parameter-Score pairs. If supplied, the process will rbind this table
to the parameter-Score pairs obtained through initialization. If Initialization is \code{FALSE}, the iterative
Gaussian fitting will be started on the leftOff table. This table should be obtained from the file saved by \code{saveIntermediate}.}

\item{parallel}{should the process run in parallel? If TRUE, several criteria must be met:
\itemize{
  \item A parallel backend must be registered
  \item \code{FUN} must be executable using the only packages specified in \code{packages} and the objects specified in \code{export}
  \item The function must be thread safe.
}}

\item{packages}{character vector of the packages needed to run \code{FUN}.}

\item{export}{character vector of object names needed to evaluate \code{FUN}.}

\item{initialize}{should the process initialize a parameter-Score pair set? If \code{FALSE}, \code{leftOff} must be provided.}

\item{initGrid}{user specified points to sample the target function, should
be a \code{data.frame} or \code{data.table} with identical column names as bounds.}

\item{initPoints}{number of randomly chosen points to sample the
scoring function before Bayesian Optimization fitting the Gaussian Process.}

\item{bulkNew}{integer that specifies the number of parameter combinations to try between each Gaussian process fit.}

\item{nIters}{total number of parameter sets to be sampled, including initial set.}

\item{kern}{a character that gets mapped to one of GauPro's \code{GauPro_kernel_beta} S6 classes.
  Determines the covariance function used in the gaussian process. Can be one of:
\itemize{
  \item \code{"Gaussian"}
  \item \code{"Exponential"}
  \item \code{"Matern52"}
  \item \code{"Matern32"}
}}

\item{beta}{the kernel lengthscale parameter log10(theta)}

\item{acq}{acquisition function type to be used. Can be "ucb", "ei", "eips" or "poi".
\itemize{
  \item \code{ucb}   Upper Confidence Bound
  \item \code{ei}    Expected Improvement
  \item \code{eips}  Expected Improvement Per Second
  \item \code{poi}   Probability of Improvement
}}

\item{stopImpatient}{a list containing \code{rounds} and \code{newAcq}, if \code{acq = "eips"} you
can switch the acquisition function to \code{newAcq} after \code{rounds} rounds}

\item{kappa}{tunable parameter kappa of GP Upper Confidence Bound, to balance exploitation against exploration,
increasing kappa will make the optimized hyperparameters pursuing exploration.}

\item{eps}{tunable parameter epsilon of Expected Improvement and Probability of Improvement, to balance exploitation against exploration,
increasing epsilon will make the optimized hyperparameters are more spread out across the whole range.}

\item{gsPoints}{integer that specifies how many initial points to try when searching for the optimal parameter set.}

\item{convThresh}{convergence threshold passed to \code{factr} when the \code{optim} function is called.
Lower values will take longer to converge, but may be more accurate.}

\item{noiseAdd}{specifies how much noise to add to the "best" parameter set found.
New random draws are pulled from a beta distribution centered at the best parameter with a range equal to
\code{noiseAdd*(Upper Bound - Lower Bound)}}

\item{verbose}{Whether or not to print progress. If 0, nothing will be printed.
If 1, progress will be printed. If 2, progress and information about new parameter-score pairs will be printed.}
}
\value{
A data.table with each parameter-Score pair.
}
\description{
Flexible Bayesian optimization of model hyperparameters.
}
\examples{
\dontrun{
require(xgboost)
data(agaricus.train, package = "xgboost")
dtrain <- xgb.DMatrix(agaricus.train$data,label = agaricus.train$label)

Folds <- list(  Fold1 = as.integer(seq(1,nrow(agaricus.train$data),by = 3))
                , Fold2 = as.integer(seq(2,nrow(agaricus.train$data),by = 3))
                , Fold3 = as.integer(seq(3,nrow(agaricus.train$data),by = 3)))

xgb_cv_bayes <- function(max_depth, min_child_weight, subsample) {

  dtrain <- xgb.DMatrix(agaricus.train$data,label = agaricus.train$label)

  cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
                             max_depth = max_depth,
                             min_child_weight = min_child_weight,
                             subsample = subsample, colsample_bytree = 0.3,
                             lambda = 1, alpha = 0,
                             objective = "binary:logistic",
                             eval_metric = "auc"),
               data = dtrain, nround = 100,
               folds = Folds, prediction = TRUE, showsd = TRUE,
               early_stopping_rounds = 5, maximize = TRUE, verbose = 0)
  return(list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration]))
}

ScoreResult <- BayesianOptimization(  FUN = xgb_cv_bayes
                                      , bounds = list(max_depth = c(2L, 6L),
                                                      min_child_weight = c(1L, 10L),
                                                      subsample = c(0.5, 0.8))
                                      , initGrid = NULL
                                      , initPoints = 10
                                      , nIters = 20
                                      , kern = Matern52$new(0)
                                      , acq = "ucb"
                                      , kappa = 2.576
                                      , verbose = TRUE
                                      , parallel = FALSE
                                      , packages = 'xgboost'
                                      , export = c('Folds','agaricus.train'))
}
}
\references{
Jasper Snoek, Hugo Larochelle, Ryan P. Adams (2012) \emph{Practical Bayesian Optimization of Machine Learning Algorithms}
}
